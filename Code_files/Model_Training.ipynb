{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "688fed3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24512, 3264),\n",
       " (1344, 3264),\n",
       " (1408, 3264),\n",
       " np.float64(0.26807278067885115),\n",
       " np.float64(0.2507440476190476),\n",
       " np.float64(0.2784090909090909))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Cell 0: Imports, paths, load features ===\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    roc_auc_score, average_precision_score, confusion_matrix\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- Paths (same BASE you used before) ----\n",
    "BASE = Path(r\"c:\\Users\\aryan\\OneDrive\\Desktop\\SEM 3\\DS 203\\DS203-Wildlife-Detection-Project\")\n",
    "DATA = BASE / \"data\"\n",
    "FEAT_DIR = DATA / \"features\"\n",
    "MODELS_DIR = BASE / \"models\"\n",
    "RESULTS_DIR = BASE / \"results\" / \"models\"\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Load features ----\n",
    "X_train = np.load(FEAT_DIR / \"X_train.npy\")\n",
    "y_train = np.load(FEAT_DIR / \"y_train.npy\")\n",
    "X_val   = np.load(FEAT_DIR / \"X_val.npy\")\n",
    "y_val   = np.load(FEAT_DIR / \"y_val.npy\")\n",
    "X_test  = np.load(FEAT_DIR / \"X_test.npy\")\n",
    "y_test  = np.load(FEAT_DIR / \"y_test.npy\")\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape, y_train.mean(), y_val.mean(), y_test.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d49efa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 1: Metrics helpers ===\n",
    "def evaluate(y_true, y_pred, y_proba=None):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "    out = {\"acc\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1}\n",
    "    if y_proba is not None:\n",
    "        try:\n",
    "            roc = roc_auc_score(y_true, y_proba)\n",
    "        except ValueError:\n",
    "            roc = np.nan\n",
    "        try:\n",
    "            ap = average_precision_score(y_true, y_proba)\n",
    "        except ValueError:\n",
    "            ap = np.nan\n",
    "        out.update({\"roc_auc\": roc, \"pr_auc\": ap})\n",
    "    return out\n",
    "\n",
    "def print_report(name, metrics):\n",
    "    msg = (f\"{name:>8s} | \"\n",
    "           f\"Acc {metrics['acc']:.4f} | P {metrics['precision']:.4f} | \"\n",
    "           f\"R {metrics['recall']:.4f} | F1 {metrics['f1']:.4f} | \"\n",
    "           f\"ROC-AUC {metrics.get('roc_auc', np.nan):.4f} | PR-AUC {metrics.get('pr_auc', np.nan):.4f}\")\n",
    "    print(msg)\n",
    "\n",
    "def plot_curves(y_true, y_proba, title_prefix):\n",
    "    from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "    plt.figure()\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
    "    plt.plot(fpr, tpr, label=\"ROC\")\n",
    "    plt.plot([0,1],[0,1],'--')\n",
    "    plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(f\"{title_prefix} - ROC\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    p, r, _ = precision_recall_curve(y_true, y_proba)\n",
    "    plt.plot(r, p, label=\"PR\")\n",
    "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(f\"{title_prefix} - PR\")\n",
    "    plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77b898b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LogReg | Acc 0.6868 | P 0.4129 | R 0.5905 | F1 0.4860 | ROC-AUC 0.6988 | PR-AUC 0.4677\n"
     ]
    }
   ],
   "source": [
    "# === Cell 2: Logistic Regression (balanced) ===\n",
    "logreg = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=True)),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        max_iter=2000,\n",
    "        class_weight=\"balanced\",\n",
    "        n_jobs=None,\n",
    "        solver=\"lbfgs\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict/Proba on VAL\n",
    "val_pred_lr = logreg.predict(X_val)\n",
    "# Proba: handle both binary and ovr shapes\n",
    "if hasattr(logreg.named_steps[\"clf\"], \"predict_proba\"):\n",
    "    val_proba_lr = logreg.predict_proba(X_val)[:, 1]\n",
    "else:\n",
    "    # decision_function fallback\n",
    "    dec = logreg.decision_function(X_val)\n",
    "    # Min-max scale to [0,1] for AUCs\n",
    "    val_proba_lr = (dec - dec.min()) / (dec.max() - dec.min() + 1e-9)\n",
    "\n",
    "m_lr = evaluate(y_val, val_pred_lr, val_proba_lr)\n",
    "print_report(\"LogReg\", m_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd74fcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RandFor | Acc 0.7708 | P 0.8537 | R 0.1039 | F1 0.1852 | ROC-AUC 0.8677 | PR-AUC 0.6631\n"
     ]
    }
   ],
   "source": [
    "# === Cell 3: RandomForest (balanced_subsample) ===\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=None,\n",
    "    n_jobs=-1,\n",
    "    class_weight=\"balanced_subsample\",\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "val_pred_rf  = rf.predict(X_val)\n",
    "val_proba_rf = rf.predict_proba(X_val)[:, 1]\n",
    "m_rf = evaluate(y_val, val_pred_rf, val_proba_rf)\n",
    "print_report(\"RandFor\", m_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acd11dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best on VAL: logreg -> {'acc': 0.6867559523809523, 'precision': 0.41286307053941906, 'recall': 0.5905044510385756, 'f1': 0.48595848595848595, 'roc_auc': 0.6988056895500045, 'pr_auc': 0.46769943574354994}\n",
      "\n",
      "    TEST | Acc 0.6761 | P 0.4389 | R 0.5867 | F1 0.5022 | ROC-AUC 0.7104 | PR-AUC 0.5065\n",
      "Saved model to: c:\\Users\\aryan\\OneDrive\\Desktop\\SEM 3\\DS 203\\DS203-Wildlife-Detection-Project\\models\\final_model.pkl\n",
      "Saved metrics to: c:\\Users\\aryan\\OneDrive\\Desktop\\SEM 3\\DS 203\\DS203-Wildlife-Detection-Project\\results\\models\\metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# === Cell 4: Select best on VAL, evaluate on TEST, save ===\n",
    "candidates = [\n",
    "    (\"logreg\", logreg, m_lr, val_proba_lr),\n",
    "    (\"rf\",     rf,     m_rf, val_proba_rf),\n",
    "]\n",
    "\n",
    "# Choose by F1 (you can switch to PR-AUC if you prefer)\n",
    "best_name, best_model, best_m, best_val_proba = max(candidates, key=lambda x: x[2][\"f1\"])\n",
    "print(f\"\\nBest on VAL: {best_name} -> {best_m}\\n\")\n",
    "\n",
    "# Test evaluation\n",
    "test_pred = best_model.predict(X_test)\n",
    "if hasattr(best_model, \"predict_proba\"):\n",
    "    test_proba = best_model.predict_proba(X_test)[:,1]\n",
    "else:\n",
    "    dec = best_model.decision_function(X_test)\n",
    "    test_proba = (dec - dec.min())/(dec.max()-dec.min()+1e-9)\n",
    "\n",
    "m_test = evaluate(y_test, test_pred, test_proba)\n",
    "print_report(\"TEST\", m_test)\n",
    "\n",
    "# Save model + metrics\n",
    "model_path = MODELS_DIR / \"final_model.pkl\"\n",
    "with open(model_path, \"wb\") as f:\n",
    "    pickle.dump(best_model, f)\n",
    "print(\"Saved model to:\", model_path)\n",
    "\n",
    "# Save metrics as CSV\n",
    "rows = []\n",
    "for name, mdl, m, _ in candidates:\n",
    "    rows.append({\"model\": name, **m, \"split\": \"val\"})\n",
    "rows.append({\"model\": best_name, **m_test, \"split\": \"test\"})\n",
    "pd.DataFrame(rows).to_csv(RESULTS_DIR / \"metrics.csv\", index=False)\n",
    "print(\"Saved metrics to:\", RESULTS_DIR / \"metrics.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6f0564f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>thr</th>\n",
       "      <th>acc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>pr_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.680060</td>\n",
       "      <td>0.410405</td>\n",
       "      <td>0.632047</td>\n",
       "      <td>0.497664</td>\n",
       "      <td>0.698806</td>\n",
       "      <td>0.467699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.655506</td>\n",
       "      <td>0.389860</td>\n",
       "      <td>0.661721</td>\n",
       "      <td>0.490649</td>\n",
       "      <td>0.698806</td>\n",
       "      <td>0.467699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.706845</td>\n",
       "      <td>0.434180</td>\n",
       "      <td>0.557864</td>\n",
       "      <td>0.488312</td>\n",
       "      <td>0.698806</td>\n",
       "      <td>0.467699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.686756</td>\n",
       "      <td>0.412863</td>\n",
       "      <td>0.590504</td>\n",
       "      <td>0.485958</td>\n",
       "      <td>0.698806</td>\n",
       "      <td>0.467699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.367250</td>\n",
       "      <td>0.685460</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.698806</td>\n",
       "      <td>0.467699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    thr       acc  precision    recall        f1   roc_auc    pr_auc\n",
       "7  0.45  0.680060   0.410405  0.632047  0.497664  0.698806  0.467699\n",
       "6  0.40  0.655506   0.389860  0.661721  0.490649  0.698806  0.467699\n",
       "9  0.55  0.706845   0.434180  0.557864  0.488312  0.698806  0.467699\n",
       "8  0.50  0.686756   0.412863  0.590504  0.485958  0.698806  0.467699\n",
       "5  0.35  0.625000   0.367250  0.685460  0.478261  0.698806  0.467699"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Cell 6 (optional): Threshold sweep to tune F1 on VAL ===\n",
    "def threshold_sweep(y_true, y_score, thr_list=None):\n",
    "    if thr_list is None:\n",
    "        thr_list = np.linspace(0.1, 0.9, 17)\n",
    "    rows = []\n",
    "    for t in thr_list:\n",
    "        yp = (y_score >= t).astype(int)\n",
    "        m = evaluate(y_true, yp, y_score)\n",
    "        rows.append({\"thr\": t, **m})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "thr_df = threshold_sweep(y_val, best_val_proba)\n",
    "thr_df.to_csv(RESULTS_DIR / \"threshold_sweep_val.csv\", index=False)\n",
    "thr_df.sort_values(\"f1\", ascending=False).head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fb4740",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
